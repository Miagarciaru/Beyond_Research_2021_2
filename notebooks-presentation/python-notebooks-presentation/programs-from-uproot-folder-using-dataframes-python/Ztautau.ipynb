{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614a26c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade --user pip # update the pip package installer\n",
    "!{sys.executable} -m pip install -U numpy pandas uproot3 matplotlib --user # install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f4452e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc10-opt/lib/python3.9/site-packages/uproot3/__init__.py:127: FutureWarning: Consider switching from 'uproot3' to 'uproot', since the new interface became the default in 2020.\n",
      "\n",
      "    pip install -U uproot\n",
      "\n",
      "In Python:\n",
      "\n",
      "    >>> import uproot\n",
      "    >>> with uproot.open(...) as file:\n",
      "    ...\n",
      "\n",
      "  warnings.warn(\n",
      "/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc10-opt/lib/python3.9/site-packages/awkward0/__init__.py:12: FutureWarning: Consider switching from 'awkward0' to 'awkward', since the new interface became the default in 2020.\n",
      "\n",
      "    pip install -U awkward\n",
      "\n",
      "In Python:\n",
      "\n",
      "    >>> import awkward as ak\n",
      "    >>> new_style_array = ak.from_awkward0(old_style_array)\n",
      "    >>> old_style_array = ak.to_awkward0(new_style_array)\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # to store data as dataframes\n",
    "import numpy as np # for numerical calculations such as histogramming\n",
    "import math # for mathematical functions such as square root\n",
    "import uproot3 # to read .root files as dataframes\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "from matplotlib.ticker import AutoMinorLocator # for minor ticks\n",
    "import infofile # local file containing info on cross-sections, sums of weights, dataset IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13897be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lumi = 0.5 # fb-1 # data_A only\n",
    "#lumi = 1.9 # fb-1 # data_B only\n",
    "#lumi = 2.9 # fb-1 # data_C only\n",
    "#lumi = 4.7 # fb-1 # data_D only\n",
    "lumi = 10 # fb-1 # data_A,data_B,data_C,data_D\n",
    "\n",
    "fraction = 1.0 # reduce this is if you want the code to run quicker\n",
    "                                                                                                                                  \n",
    "#tuple_path = \"Input/4lep/\" # local \n",
    "tuple_path = \"https://atlas-opendata.web.cern.ch/atlas-opendata/samples/2020/1lep1tau/\" # web address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2675a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\n",
    "\n",
    "    'data': {\n",
    "        'list' : ['data_A','data_B','data_C','data_D'],\n",
    "    },\n",
    "\n",
    "    r'Background $diboson$' : { # \n",
    "        'list' : ['ZqqZll','WqqZll','WpqqWmlv', 'WplvWmqq', 'WlvZqq', 'llll', 'lllv', 'llvv', 'lvvv'],\n",
    "        'color' : \"#6b59d3\" # purple\n",
    "    },\n",
    "\n",
    "    r'Background $single top$' : { # \n",
    "        'list' : ['single_top_tchan','single_antitop_tchan','single_top_schan','single_antitop_schan','single_top_wtchan','single_antitop_wtchan'],\n",
    "        'color' : \"#ff0000\" # red\n",
    "    },\n",
    "    \n",
    "    \n",
    "    r'Background ($t\\bar{t}$)' : { #\n",
    "        'list' : ['ttbar_lep'],\n",
    "        'color' : \"#00cdff\" # light blue\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8532dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_files():\n",
    "\n",
    "    data = {} # define empty dictionary to hold dataframes\n",
    "    for s in samples: # loop over samples\n",
    "        print('Processing '+s+' samples') # print which sample\n",
    "        frames = [] # define empty list to hold data\n",
    "        for val in samples[s]['list']: # loop over each file\n",
    "            if s == 'data': prefix = \"Data/\" # Data prefix\n",
    "            else: # MC prefix\n",
    "                prefix = \"MC/mc_\"+str(infofile.infos[val][\"DSID\"])+\".\"\n",
    "            fileString = tuple_path+prefix+val+\".1lep1tau.root\" # file name to open\n",
    "            temp = read_file(fileString,val) # call the function read_file defined below\n",
    "            frames.append(temp) # append dataframe returned from read_file to list of dataframes\n",
    "            #print(temp['lep_pt'])\n",
    "            #print(temp['mllll'])\n",
    "        data[s] = pd.concat(frames) # dictionary entry is concatenated dataframes\n",
    "    \n",
    "    return data # return dictionary of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91b9f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_weight(xsec_weight, mcWeight, scaleFactor_PILEUP,\n",
    "                scaleFactor_ELE, scaleFactor_MUON, \n",
    "                scaleFactor_LepTRIGGER ):\n",
    "    return (xsec_weight * mcWeight * scaleFactor_PILEUP * scaleFactor_ELE * scaleFactor_MUON  * scaleFactor_LepTRIGGER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec143539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xsec_weight(sample):\n",
    "    info = infofile.infos[sample] # open infofile\n",
    "    xsec_weight = (lumi*1000*info[\"xsec\"])/(info[\"sumw\"]*info[\"red_eff\"]) #*1000 to go from fb-1 to pb-1\n",
    "    return xsec_weight # return cross-section weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f4b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger(trigE, trigM):\n",
    "    if((trigE != True) and (trigM != True)): return True\n",
    "    return False\n",
    "\n",
    "def good_lep_index(lep_n, lep_pt, lep_eta, lep_ptcone30, lep_etcone20, lep_type, lep_isTightID):\n",
    "    goodlep_n = 0\n",
    "    goodlep_index = 0\n",
    "    lep_index = 0\n",
    "    list_lep_index = [0, 0, 0]\n",
    "    for i in lep_n:\n",
    "        if(lep_isTightID[i]==False): continue\n",
    "        #Lepton is highly isolated and hard\n",
    "        if(lep_pt[i]<=30000. or (lep_ptcone30[i]/lep_pt[i]>=0.1) or (lep_etcone20[i]/lep_pt[i]>=0.1) ): continue\n",
    "        # electron selection in fiducial region excluding candidates in the transition region between \n",
    "        #the barrel and endcap electromagnetic calorimeters\n",
    "        if(lep_type[i]==11 and abs(lep_eta[i])<2.47 and (abs(lep_eta[i])<1.37 or abs(lep_eta[i])>1.52)):\n",
    "            goodlep_n+=1\n",
    "            goodlep_index = i\n",
    "            lep_index+=1\n",
    "        # muon selection\n",
    "        if(lep_type[i]==13 and abs(lep_eta[i])<2.5):\n",
    "            goodlep_n+=1\n",
    "            goodlep_index = i\n",
    "            lep_index+=1\n",
    "                    \n",
    "    if(goodlep_n==1):\n",
    "        list_lep_index = [goodlep_n, goodlep_index, lep_index]\n",
    "    \n",
    "    return list_lep_index\n",
    "\n",
    "def goodlep_cut(lep_index_list):\n",
    "    goodlep_n = lep_index_list[0]\n",
    "    if(goodlep_n != 1): return True\n",
    "    return False\n",
    "\n",
    "def good_tau_index(tau_n, tau_pt, tau_eta, tau_isTightID):\n",
    "    goodtau_n = 0\n",
    "    goodtau_index = 0\n",
    "    tau_index = 0\n",
    "    list_tau_index = [0, 0, 0]\n",
    "    for i in tau_n:\n",
    "        if(tau_isTightID == False): continue\n",
    "        if(tau_pt[i]>25000. and abs(tau_eta[i])<2.5):\n",
    "            goodtau_n+=1\n",
    "            goodtau_index = i\n",
    "            tau_index+=1\n",
    "                \n",
    "    if(goodtau_n==1):\n",
    "        list_tau_index = [goodtau_n, goodtau_index, tau_index]\n",
    "        \n",
    "    return list_tau_index\n",
    "\n",
    "def goodtau_cut(goodtau_index_list):\n",
    "    gtau_n = goodtau_index_list[0]\n",
    "    if(gtau_n != 1): return True\n",
    "    return False\n",
    "\n",
    "def opposite_charge_cut(lep_charge, tau_charge, goodlep_index, goodtau_index):\n",
    "    glep_index = goodlep_index[1]\n",
    "    gtau_index = goodtau_index[1]\n",
    "    if(lep_charge[glep_index]*tau_charge[gtau_index]>0): return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5dc9517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looks good\n",
    "def trigger(trigE, trigM):\n",
    "    if((trigE != True) and (trigM != True)): return True\n",
    "    return False\n",
    "\n",
    "def goodlep_cut(lep_n, lep_pt, lep_eta, lep_ptcone30, lep_etcone20, lep_type, lep_isTightID):\n",
    "    goodlep_n = 0\n",
    "    goodlep_index = 0\n",
    "    lep_index = 0\n",
    "    list_lep_index = [0, 0, 0]\n",
    "    for i in range (lep_n):\n",
    "        if(lep_isTightID[i]==False): continue\n",
    "        #Lepton is highly isolated and hard\n",
    "        if(lep_pt[i]<=30000. or (lep_ptcone30[i]/lep_pt[i]>=0.1) or (lep_etcone20[i]/lep_pt[i]>=0.1) ): continue\n",
    "        # electron selection in fiducial region excluding candidates in the transition region between \n",
    "        #the barrel and endcap electromagnetic calorimeters\n",
    "        if(lep_type[i]==11 and abs(lep_eta[i])<2.47 and (abs(lep_eta[i])<1.37 or abs(lep_eta[i])>1.52)):\n",
    "            goodlep_n+=1\n",
    "            goodlep_index = i\n",
    "            lep_index+=1\n",
    "        # muon selection\n",
    "        if(lep_type[i]==13 and abs(lep_eta[i])<2.5):\n",
    "            goodlep_n+=1\n",
    "            goodlep_index = i\n",
    "            lep_index+=1\n",
    "                    \n",
    "    if(goodlep_n!=1): return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def goodtau_cut(tau_n, tau_pt, tau_eta, tau_isTightID):\n",
    "    goodtau_n = 0\n",
    "    goodtau_index = 0\n",
    "    tau_index = 0\n",
    "    list_tau_index = [0, 0, 0]\n",
    "    for i in range (tau_n):\n",
    "        if(tau_isTightID[i] == False): continue\n",
    "        if(tau_pt[i]>25000. and abs(tau_eta[i])<2.5):\n",
    "            goodtau_n+=1\n",
    "            goodtau_index = i\n",
    "            tau_index+=1\n",
    "                \n",
    "    if(goodtau_n==1): return True\n",
    "    return False\n",
    "\n",
    "def opposite_charge_cut(lep_charge, tau_charge, goodlep_index, goodtau_index):\n",
    "    glep_index = goodlep_index[1]\n",
    "    gtau_index = goodtau_index[1]\n",
    "    if(lep_charge[glep_index]*tau_charge[gtau_index]>0): return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2836a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [0, 0, 0];\n",
    "def attend(list):\n",
    "    list = [7,2,3]\n",
    "    return list\n",
    "\n",
    "def cut(list):\n",
    "    number = list[0]\n",
    "    print(number)\n",
    "    if(number == 1): return True\n",
    "    return False\n",
    "print(list1)\n",
    "list1 = attend(list1)\n",
    "n = list1[0]\n",
    "print(n)\n",
    "print(list1)\n",
    "print(cut(list1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62503a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks good\n",
    "def read_file(path,sample):\n",
    "    \n",
    "    print(\"\\tProcessing: \"+sample) # print which sample is being processed\n",
    "    data_all = pd.DataFrame() # define empty pandas DataFrame to hold all data for this sample\n",
    "    tree = uproot3.open(path)[\"mini\"] # open the tree called mini\n",
    "    numevents = uproot3.numentries(path, \"mini\") # number of events\n",
    "    if 'data' not in sample: xsec_weight = get_xsec_weight(sample) # get cross-section weight\n",
    "    for data in tree.iterate(['trigE','trigM','lep_isTightID','lep_n','lep_charge','lep_type','lep_pt', 'lep_ptcone30', \n",
    "                              'lep_etcone20','lep_eta','lep_phi','lep_E', 'tau_charge', 'tau_n', 'tau_pt', 'tau_eta', \n",
    "                              'tau_isTightID','mcWeight','scaleFactor_PILEUP',\n",
    "                              'scaleFactor_ELE','scaleFactor_MUON',\n",
    "                              'scaleFactor_LepTRIGGER'], # variables to calculate Monte Carlo weight\n",
    "                             outputtype=pd.DataFrame, # choose output type as pandas DataFrame\n",
    "                             entrystop=numevents*fraction): # process up to numevents*fraction\n",
    "\n",
    "        nIn = len(data.index) # number of events in this batch\n",
    " \n",
    "        if 'data' not in sample: # only do this for Monte Carlo simulation files\n",
    "            # multiply all Monte Carlo weights and scale factors together to give total weight\n",
    "        \n",
    "            data['totalWeight'] = np.vectorize(calc_weight)(xsec_weight,\n",
    "                                                            data.mcWeight,\n",
    "                                                            data.scaleFactor_PILEUP,\n",
    "                                                            data.scaleFactor_ELE,\n",
    "                                                            data.scaleFactor_MUON,\n",
    "                                                            data.scaleFactor_LepTRIGGER)\n",
    "\n",
    "        #cut on trigger\n",
    "        fail = data[ np.vectorize(trigger)(data.trigE, data.trigM) ].index # get events that fail this selection\n",
    "        data.drop(fail,inplace=True) # drop the events with fewer than 1 b-jets\n",
    "        if len(data.index)==0: continue # move onto next batch if no events left\n",
    "            \n",
    "        #lep_index = [0, 0, 0]\n",
    "        #lep_index = good_lep_index(data.lep_n, data.lep_pt, data.lep_eta, data.lep_ptcone30, data.lep_etcone20, data.lep_type, \n",
    "                                   #data.lep_isTightID)\n",
    "        \n",
    "        #data['goodlep_index'] = lep_index\n",
    "\n",
    "        #cut on goodlepton\n",
    "        fail = data[ np.vectorize(goodlep_cut)(data.lep_n, data.lep_pt, data.lep_eta, data.lep_ptcone30, \n",
    "                                               data.lep_etcone20, data.lep_type, data.lep_isTightID) ].index # get events that fail this selection\n",
    "        data.drop(fail,inplace=True) # drop the events with fewer than 1 b-jets\n",
    "        if len(data.index)==0: continue # move onto next batch if no events left\n",
    "        \n",
    "        #tau_index = [0, 0, 0]\n",
    "        #tau_index = good_tau_index(data.tau_n, data.tau_pt, data.tau_eta, data.tau_isTightID)\n",
    "        \n",
    "        #data['goodtau_index'] = tau_index\n",
    "        \n",
    "        #cut on goodtau\n",
    "        fail = data[ np.vectorize(goodtau_cut)(data.tau_n, data.tau_pt, data.tau_eta, data.tau_isTightID) ].index # get events that fail this selection\n",
    "        data.drop(fail,inplace=True) # drop the events with fewer than 1 b-jets\n",
    "        if len(data.index)==0: continue # move onto next batch if no events left \n",
    "        \n",
    "        #cut on opposite charges\n",
    "        #fail = data[ np.vectorize(opposite_charge_cut)(data.lep_charge, data.tau_charge, data.goodlep_index, \n",
    "                                                       #data.goodtau_index) ].index # get events that fail this selection\n",
    "        #data.drop(fail,inplace=True) # drop the events with fewer than 1 b-jets\n",
    "        #if len(data.index)==0: continue # move onto next batch if no events left \n",
    "        \n",
    "        nOut = len(data.index) # number of events passing cuts in this batch\n",
    "        data_all = data_all.append(data) # append dataframe from this batch to the dataframe for the whole sample\n",
    "        #print(data[['lep_pt', 'mllll']])\n",
    "        print(\"\\t\\t nIn: \"+str(nIn)+\",\\t nOut: \\t\"+str(nOut)) # events before and after\n",
    "\n",
    "    return data_all # return dataframe containing events passing all cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db4c6313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path,sample):\n",
    "    \n",
    "    print(\"\\tProcessing: \"+sample) # print which sample is being processed\n",
    "    data_all = pd.DataFrame() # define empty pandas DataFrame to hold all data for this sample\n",
    "    tree = uproot3.open(path)[\"mini\"] # open the tree called mini\n",
    "    numevents = uproot3.numentries(path, \"mini\") # number of events\n",
    "    if 'data' not in sample: xsec_weight = get_xsec_weight(sample) # get cross-section weight\n",
    "    \n",
    "    entrystop=numevents*fraction # stop after fraction of events we want to process\n",
    "    branches = ['trigE','trigM','lep_isTightID','lep_n','lep_charge','lep_type','lep_pt', 'lep_ptcone30', \n",
    "                'lep_etcone20','lep_eta','lep_phi','lep_E', 'tau_charge', 'tau_n', 'tau_pt', 'tau_eta', \n",
    "                'tau_isTightID', 'met_et'] # uncomment this for stricter lepton requirements\n",
    "               \n",
    "    if 'data' not in sample: \n",
    "        xsec_weight = get_xsec_weight(sample) #  get cross-section weight\n",
    "        branches.extend(['mcWeight','scaleFactor_PILEUP','scaleFactor_ELE',\n",
    "                         'scaleFactor_MUON','scaleFactor_LepTRIGGER'])\n",
    "            \n",
    "    for data in tree.iterate(branches, \n",
    "                             outputtype=pd.DataFrame, # choose output type as pandas DataFrame\n",
    "                             entrystop=entrystop): # process up to numevents*fraction\n",
    "\n",
    "        nIn = len(data.index) # number of events in this batch\n",
    " \n",
    "        if 'data' not in sample: # only do this for Monte Carlo simulation files\n",
    "            # multiply all Monte Carlo weights and scale factors together to give total weight\n",
    "        \n",
    "            data['totalWeight'] = np.vectorize(calc_weight)(xsec_weight,\n",
    "                                                            data.mcWeight,\n",
    "                                                            data.scaleFactor_PILEUP,\n",
    "                                                            data.scaleFactor_ELE,\n",
    "                                                            data.scaleFactor_MUON,\n",
    "                                                            data.scaleFactor_LepTRIGGER)\n",
    "\n",
    "        #cut on trigger\n",
    "        fail = data[ np.vectorize(trigger)(data.trigE, data.trigM) ].index # get events that fail this selection\n",
    "        data.drop(fail,inplace=True) # drop the events with fewer than 1 b-jets\n",
    "        if len(data.index)==0: continue # move onto next batch if no events left\n",
    "\n",
    "        #cut on goodlepton\n",
    "        fail = data[ np.vectorize(goodlep_cut)(data.lep_n, data.lep_pt, data.lep_eta, data.lep_ptcone30, \n",
    "                                               data.lep_etcone20, data.lep_type, data.lep_isTightID) ].index # get events that fail this selection\n",
    "        data.drop(fail,inplace=True) # drop the events with fewer than 1 b-jets\n",
    "        if len(data.index)==0: continue # move onto next batch if no events left\n",
    "        \n",
    "        #cut on goodtau\n",
    "        fail = data[ np.vectorize(goodtau_cut)(data.tau_n, data.tau_pt, data.tau_eta, data.tau_isTightID) ].index # get events that fail this selection\n",
    "        data.drop(fail,inplace=True) # drop the events with fewer than 1 b-jets\n",
    "        if len(data.index)==0: continue # move onto next batch if no events left \n",
    "        \n",
    "        #cut on opposite charges\n",
    "        #fail = data[ np.vectorize(opposite_charge_cut)(data.lep_charge, data.tau_charge, data.goodlep_index, \n",
    "                                                       #data.goodtau_index) ].index # get events that fail this selection\n",
    "        #data.drop(fail,inplace=True) # drop the events with fewer than 1 b-jets\n",
    "        #if len(data.index)==0: continue # move onto next batch if no events left \n",
    "        \n",
    "        nOut = len(data.index) # number of events passing cuts in this batch\n",
    "        data_all = data_all.append(data) # append dataframe from this batch to the dataframe for the whole sample\n",
    "       \n",
    "        print(\"\\t\\t nIn: \"+str(nIn)+\",\\t nOut: \\t\"+str(nOut)) # events before and after\n",
    "\n",
    "    return data_all # return dataframe containing events passing all cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceefcbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data samples\n",
      "\tProcessing: data_A\n",
      "\t\t nIn: 48826,\t nOut: \t19415\n",
      "\tProcessing: data_B\n",
      "\t\t nIn: 120433,\t nOut: \t48689\n",
      "\t\t nIn: 50265,\t nOut: \t20567\n",
      "\tProcessing: data_C\n",
      "\t\t nIn: 120433,\t nOut: \t48625\n",
      "\t\t nIn: 106825,\t nOut: \t43356\n",
      "\tProcessing: data_D\n",
      "\t\t nIn: 119700,\t nOut: \t53146\n",
      "\t\t nIn: 119700,\t nOut: \t53461\n",
      "\t\t nIn: 119700,\t nOut: \t54340\n",
      "\t\t nIn: 15018,\t nOut: \t6527\n",
      "Processing Background $diboson$ samples\n",
      "\tProcessing: ZqqZll\n",
      "\t\t nIn: 54512,\t nOut: \t18569\n",
      "\tProcessing: WqqZll\n",
      "\t\t nIn: 56044,\t nOut: \t19189\n",
      "\tProcessing: WpqqWmlv\n",
      "\t\t nIn: 27889,\t nOut: \t15996\n",
      "\tProcessing: WplvWmqq\n",
      "\t\t nIn: 28172,\t nOut: \t15919\n",
      "\tProcessing: WlvZqq\n",
      "\t\t nIn: 25604,\t nOut: \t14419\n",
      "\tProcessing: llll\n",
      "\t\t nIn: 93969,\t nOut: \t19977\n",
      "\t\t nIn: 93969,\t nOut: \t20060\n",
      "\t\t nIn: 93969,\t nOut: \t19706\n",
      "\t\t nIn: 72727,\t nOut: \t15372\n",
      "\tProcessing: lllv\n",
      "\t\t nIn: 86295,\t nOut: \t20459\n",
      "\t\t nIn: 86295,\t nOut: \t20643\n",
      "\t\t nIn: 86295,\t nOut: \t20353\n",
      "\t\t nIn: 86295,\t nOut: \t20429\n",
      "\t\t nIn: 86295,\t nOut: \t20719\n",
      "\t\t nIn: 12662,\t nOut: \t2984\n",
      "\tProcessing: llvv\n",
      "\t\t nIn: 108214,\t nOut: \t31998\n",
      "\t\t nIn: 108214,\t nOut: \t32035\n",
      "\t\t nIn: 108214,\t nOut: \t32167\n",
      "\t\t nIn: 108214,\t nOut: \t32331\n",
      "\t\t nIn: 24800,\t nOut: \t7362\n",
      "\tProcessing: lvvv\n",
      "\t\t nIn: 11353,\t nOut: \t6738\n",
      "Processing Background $single top$ samples\n",
      "\tProcessing: single_top_tchan\n",
      "\t\t nIn: 4202,\t nOut: \t2322\n",
      "\tProcessing: single_antitop_tchan\n",
      "\t\t nIn: 4911,\t nOut: \t2659\n",
      "\tProcessing: single_top_schan\n",
      "\t\t nIn: 1785,\t nOut: \t1022\n",
      "\tProcessing: single_antitop_schan\n",
      "\t\t nIn: 1897,\t nOut: \t1072\n",
      "\tProcessing: single_top_wtchan\n",
      "\t\t nIn: 12785,\t nOut: \t4872\n",
      "\tProcessing: single_antitop_wtchan\n",
      "\t\t nIn: 12564,\t nOut: \t4758\n",
      "Processing Background ($t\\bar{t}$) samples\n",
      "\tProcessing: ttbar_lep\n",
      "\t\t nIn: 85777,\t nOut: \t33896\n",
      "\t\t nIn: 85777,\t nOut: \t33592\n",
      "\t\t nIn: 85777,\t nOut: \t33618\n",
      "\t\t nIn: 85777,\t nOut: \t33731\n",
      "\t\t nIn: 85777,\t nOut: \t33779\n",
      "\t\t nIn: 85777,\t nOut: \t33617\n",
      "\t\t nIn: 14403,\t nOut: \t5707\n"
     ]
    }
   ],
   "source": [
    "data = get_data_from_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6e31ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data):\n",
    "    GeV = 1.0\n",
    "    xmin = 0 * GeV\n",
    "    xmax = 150 * GeV\n",
    "    step_size = 5 * GeV\n",
    "\n",
    "    bin_edges = np.arange(start=xmin, # The interval includes this value\n",
    "                     stop=xmax+step_size, # The interval doesn't include this value\n",
    "                     step=step_size ) # Spacing between values\n",
    "    bin_centres = np.arange(start=xmin+step_size/2, # The interval includes this value\n",
    "                            stop=xmax+step_size/2, # The interval doesn't include this value\n",
    "                            step=step_size ) # Spacing between values\n",
    "\n",
    "    data_x,_ = np.histogram(data['data']['met_et']/1000., \n",
    "                            bins=bin_edges ) # histogram the data\n",
    "    data_x_errors = np.sqrt( data_x ) # statistical error on the data\n",
    "\n",
    "    signal_x = data[r'Signal ($m_H$ = 125 GeV)']['met_et']/1000. # histogram the signal\n",
    "    signal_weights = data[r'Signal ($m_H$ = 125 GeV)'].totalWeight # get the weights of the signal events\n",
    "    signal_color = samples[r'Signal ($m_H$ = 125 GeV)']['color'] # get the colour for the signal bar\n",
    "\n",
    "    mc_x = [] # define list to hold the Monte Carlo histogram entries\n",
    "    mc_weights = [] # define list to hold the Monte Carlo weights\n",
    "    mc_colors = [] # define list to hold the colors of the Monte Carlo bars\n",
    "    mc_labels = [] # define list to hold the legend labels of the Monte Carlo bars\n",
    "\n",
    "    for s in samples: # loop over samples\n",
    "        if s not in ['data']: # if not data nor signal\n",
    "            mc_x.append( data[s]['met_et']/1000. ) # append to the list of Monte Carlo histogram entries\n",
    "            mc_weights.append( data[s].totalWeight ) # append to the list of Monte Carlo weights\n",
    "            mc_colors.append( samples[s]['color'] ) # append to the list of Monte Carlo bar colors\n",
    "            mc_labels.append( s ) # append to the list of Monte Carlo legend labels\n",
    "    \n",
    "\n",
    "\n",
    "    # *************\n",
    "    # Main plot \n",
    "    # *************\n",
    "    main_axes = plt.gca() # get current axes\n",
    "    \n",
    "    # plot the data points\n",
    "    main_axes.errorbar(x=bin_centres, y=data_x, yerr=data_x_errors,\n",
    "                      fmt='ko', # 'k' means black and 'o' is for circles \n",
    "                      label='Data') \n",
    "    \n",
    "    # plot the Monte Carlo bars\n",
    "    mc_heights = main_axes.hist(mc_x, bins=bin_edges, \n",
    "                                weights=mc_weights, stacked=True, \n",
    "                                color=mc_colors, label=mc_labels )\n",
    "    \n",
    "    mc_x_tot = mc_heights[0][-1] # stacked background MC y-axis value\n",
    "    \n",
    "    # calculate MC statistical uncertainty: sqrt(sum w^2)\n",
    "    mc_x_err = np.sqrt(np.histogram(np.hstack(mc_x), bins=bin_edges, weights=np.hstack(mc_weights)**2)[0])\n",
    "    \n",
    "    # plot the signal bar\n",
    "    #main_axes.hist(signal_x, bins=bin_edges, bottom=mc_x_tot, \n",
    "    #              weights=signal_weights, color=signal_color,\n",
    "    #               label=r'Signal ($m_H$ = 125 GeV)')\n",
    "    \n",
    "    # plot the statistical uncertainty\n",
    "    main_axes.bar(bin_centres, # x\n",
    "                  2*mc_x_err, # heights\n",
    "                  alpha=0.5, # half transparency\n",
    "                  bottom=mc_x_tot-mc_x_err, color='none', \n",
    "                  hatch=\"////\", width=step_size, label='Stat. Unc.' )\n",
    "\n",
    "    # set the x-limit of the main axes\n",
    "    main_axes.set_xlim( left=xmin, right=xmax ) \n",
    "    \n",
    "    # separation of x axis minor ticks\n",
    "    main_axes.xaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "    \n",
    "    # set the axis tick parameters for the main axes\n",
    "    main_axes.tick_params(which='both', # ticks on both x and y axes\n",
    "                          direction='in', # Put ticks inside and outside the axes\n",
    "                          top=True, # draw ticks on the top axis\n",
    "                          right=True ) # draw ticks on right axis\n",
    "    \n",
    "    # x-axis label\n",
    "    main_axes.set_xlabel(r'4-lepton invariant mass $\\mathrm{m_{4l}}$ [GeV]',\n",
    "                        fontsize=13, x=1, horizontalalignment='right' )\n",
    "    \n",
    "    # write y-axis label for main axes\n",
    "    main_axes.set_ylabel('Events / '+str(step_size)+' GeV',\n",
    "                         y=1, horizontalalignment='right') \n",
    "    \n",
    "    # set y-axis limits for main axes\n",
    "    main_axes.set_ylim( bottom=0, top=np.amax(data_x)*1.6 )\n",
    "    \n",
    "    # add minor ticks on y-axis for main axes\n",
    "    main_axes.yaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "\n",
    "    # Add text 'ATLAS Open Data' on plot\n",
    "    plt.text(0.05, # x\n",
    "             0.93, # y\n",
    "             'ATLAS Open Data', # text\n",
    "             transform=main_axes.transAxes, # coordinate system used is that of main_axes\n",
    "             fontsize=13 ) \n",
    "    \n",
    "    # Add text 'for education' on plot\n",
    "    plt.text(0.05, # x\n",
    "             0.88, # y\n",
    "             'for education', # text\n",
    "             transform=main_axes.transAxes, # coordinate system used is that of main_axes\n",
    "             style='italic',\n",
    "             fontsize=8 ) \n",
    "    \n",
    "    # Add energy and luminosity\n",
    "    lumi_used = str(lumi*fraction) # luminosity to write on the plot\n",
    "    plt.text(0.05, # x\n",
    "             0.82, # y\n",
    "             '$\\sqrt{s}$=13 TeV,$\\int$L dt = '+lumi_used+' fb$^{-1}$', # text\n",
    "             transform=main_axes.transAxes ) # coordinate system used is that of main_axes\n",
    "    \n",
    "    # Add a label for the analysis carried out\n",
    "    plt.text(0.05, # x\n",
    "             0.76, # y\n",
    "             r'$H \\rightarrow ZZ^* \\rightarrow 4\\ell$', # text \n",
    "             transform=main_axes.transAxes ) # coordinate system used is that of main_axes\n",
    "\n",
    "    # draw the legend\n",
    "    main_axes.legend( frameon=False ) # no box around the legend\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62e0449f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'met_et'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc10-opt/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'met_et'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1579/3264040271.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1579/1389721658.py\u001b[0m in \u001b[0;36mplot_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     12\u001b[0m                             step=step_size ) # Spacing between values\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     data_x,_ = np.histogram(data['data']['met_et']/1000., \n\u001b[0m\u001b[1;32m     15\u001b[0m                             bins=bin_edges ) # histogram the data\n\u001b[1;32m     16\u001b[0m     \u001b[0mdata_x_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdata_x\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;31m# statistical error on the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc10-opt/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc10-opt/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'met_et'"
     ]
    }
   ],
   "source": [
    "plot_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c124b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
